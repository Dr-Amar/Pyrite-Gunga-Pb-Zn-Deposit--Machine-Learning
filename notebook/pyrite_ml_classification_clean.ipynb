{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read your Excel file into a Pandas DataFrame\n",
    "df = pd.read_excel('/home/alifian/Geology Data Analysis/Pyrite_-21-feb(Final Version) copy.xlsx')\n",
    "# Specify columns to transform\n",
    "columns_to_transform = ['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']\n",
    "\n",
    "# Apply natural logarithm to selected columns\n",
    "df[columns_to_transform] = np.log(df[columns_to_transform])\n",
    "\n",
    "# Calculate the mean and standard deviation of the log-transformed columns\n",
    "mean = df[columns_to_transform].mean()\n",
    "std = df[columns_to_transform].std()\n",
    "\n",
    "# Standardize the log-transformed columns using the mean and standard deviation\n",
    "df[columns_to_transform] = (df[columns_to_transform] - mean) / std\n",
    "\n",
    "# Round the values to four decimal places\n",
    "df = df.round(4)\n",
    "\n",
    "# Save the result to a new Excel file\n",
    "df.to_excel('Pyrite_Standarized_data_file.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)  # Apply the imputer to the X DataFrame\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier with specified parameters\n",
    "rf_classifier = RandomForestClassifier(n_estimators=400, max_depth=20, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = rf_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the SVM classifier with specified parameters\n",
    "svm_classifier = SVC(C=100, kernel='rbf', gamma=0.1)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = svm_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the MLP classifier with specified parameters\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(150, 100, 50), max_iter=300, activation='tanh', solver='adam', alpha=0.0001, learning_rate='constant')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = mlp_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = mlp_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = gb_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = gb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance for RF and Gradient Boost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "features = ['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']\n",
    "X = df[features]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier with specified parameters\n",
    "rf_classifier = RandomForestClassifier(n_estimators=400, max_depth=20, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Print feature importances\n",
    "for feature, importance in zip(features, importances):\n",
    "    print(f'The importance of {feature} is: {importance:.4f}')\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = rf_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "features = ['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']\n",
    "X = df[features]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(learning_rate=0.1, max_depth=5, n_estimators=300, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = gb_classifier.feature_importances_\n",
    "\n",
    "# Print feature importances\n",
    "for feature, importance in zip(features, importances):\n",
    "    print(f'The importance of {feature} is: {importance:.4f}')\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = gb_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.4f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = gb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)  # Apply the imputer to the X DataFrame\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [5, 10, 15, 20, 25, 30],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_cv = GridSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_grid=hyperparameter_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the classifier on the training data\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_cv.best_params_\n",
    "print(f'Best parameters: {best_params}')\n",
    "\n",
    "# Make predictions on the validation set using the model with the best parameters\n",
    "y_val_pred = grid_cv.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = grid_cv.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_cv = GridSearchCV(\n",
    "    estimator=svm_classifier,\n",
    "    param_grid=hyperparameter_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the classifier on the training data\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_cv.best_params_\n",
    "print(f'Best parameters: {best_params}')\n",
    "\n",
    "# Make predictions on the validation set using the model with the best parameters\n",
    "y_val_pred = grid_cv.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = grid_cv.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the MLP classifier\n",
    "mlp_classifier = MLPClassifier(max_iter=300)\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_cv = GridSearchCV(\n",
    "    estimator=mlp_classifier,\n",
    "    param_grid=hyperparameter_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the classifier on the training data\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_cv.best_params_\n",
    "print(f'Best parameters: {best_params}')\n",
    "\n",
    "# Make predictions on the validation set using the model with the best parameters\n",
    "y_val_pred = grid_cv.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = grid_cv.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters for Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_cv = GridSearchCV(\n",
    "    estimator=gb_classifier,\n",
    "    param_grid=hyperparameter_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the classifier on the training data\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_cv.best_params_\n",
    "print(f'Best parameters: {best_params}')\n",
    "\n",
    "# Make predictions on the validation set using the model with the best parameters\n",
    "y_val_pred = grid_cv.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = grid_cv.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelBinarizerca\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)  # Apply the imputer to the X DataFrame\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier with specified parameters\n",
    "rf_classifier = RandomForestClassifier(n_estimators=400, max_depth=20, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = rf_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Calculate AUC on the validation set\n",
    "y_val_pred_proba = rf_classifier.predict_proba(X_val)\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_val)\n",
    "y_val_lb = lb.transform(y_val)\n",
    "val_auc = roc_auc_score(y_val_lb, y_val_pred_proba, multi_class='ovr')\n",
    "print(f'Validation AUC: {val_auc:.4f}')\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)\n",
    "\n",
    "# Calculate AUC on the test set\n",
    "y_test_pred_proba = rf_classifier.predict_proba(X_test)\n",
    "y_test_lb = lb.transform(y_test)\n",
    "test_auc = roc_auc_score(y_test_lb, y_test_pred_proba, multi_class='ovr')\n",
    "print(f'Test AUC: {test_auc:.4f}')\n",
    "\n",
    "# Output number of original and predicted type counts for validation set\n",
    "val_original_counts = y_val.value_counts()\n",
    "val_predicted_counts = pd.Series(y_val_pred).value_counts()\n",
    "print('\\nNumber of Original Type Counts (Validation Set):')\n",
    "print(val_original_counts)\n",
    "print('\\nNumber of Predicted Type Counts (Validation Set):')\n",
    "print(val_predicted_counts)\n",
    "\n",
    "# Output number of original and predicted type counts for test set\n",
    "test_original_counts = y_test.value_counts()\n",
    "test_predicted_counts = pd.Series(y_test_pred).value_counts()\n",
    "print('\\nNumber of Original Type Counts (Test Set):')\n",
    "print(test_original_counts)\n",
    "print('\\nNumber of Predicted Type Counts (Test Set):')\n",
    "print(test_predicted_counts)\n",
    "\n",
    "# Calculate precision score\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "print(f'Precision Score: {precision:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the SVM classifier with specified parameters\n",
    "svm_classifier = SVC(C=100, kernel='rbf', gamma=0.1, probability=True)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = svm_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Calculate AUC on the validation set\n",
    "y_val_pred_proba = svm_classifier.predict_proba(X_val)\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_val)\n",
    "y_val_lb = lb.transform(y_val)\n",
    "val_auc = roc_auc_score(y_val_lb, y_val_pred_proba, multi_class='ovr')\n",
    "print(f'Validation AUC: {val_auc:.4f}')\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)\n",
    "\n",
    "# Calculate AUC on the test set\n",
    "y_test_pred_proba = svm_classifier.predict_proba(X_test)\n",
    "y_test_lb = lb.transform(y_test)\n",
    "test_auc = roc_auc_score(y_test_lb, y_test_pred_proba, multi_class='ovr')\n",
    "print(f'Test AUC: {test_auc:.4f}')\n",
    "\n",
    "# Output number of original and predicted type counts for validation set\n",
    "val_original_counts = y_val.value_counts()\n",
    "val_predicted_counts = pd.Series(y_val_pred).value_counts()\n",
    "print('\\nNumber of Original Type Counts (Validation Set):')\n",
    "print(val_original_counts)\n",
    "print('\\nNumber of Predicted Type Counts (Validation Set):')\n",
    "print(val_predicted_counts)\n",
    "\n",
    "# Output number of original and predicted type counts for test set\n",
    "test_original_counts = y_test.value_counts()\n",
    "test_predicted_counts = pd.Series(y_test_pred).value_counts()\n",
    "print('\\nNumber of Original Type Counts (Test Set):')\n",
    "print(test_original_counts)\n",
    "print('\\nNumber of Predicted Type Counts (Test Set):')\n",
    "print(test_predicted_counts)\n",
    "\n",
    "# Calculate precision score\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "print(f'Precision Score: {precision:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC of MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import aaccuracy_score, classification_report, confusion_matrix, precision_score, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the MLP classifier with specified parameters\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(50, 100, 50), max_iter=300, activation='tanh', solver='adam', alpha=0.0001, learning_rate='constant')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = mlp_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Calculate AUC on the validation set\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_val)\n",
    "y_val_lb = lb.transform(y_val)\n",
    "y_val_pred_proba = mlp_classifier.predict_proba(X_val)\n",
    "val_auc = roc_auc_score(y_val_lb, y_val_pred_proba, multi_class='ovr')\n",
    "print(f'Validation AUC: {val_auc:.4f}')\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = mlp_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)\n",
    "\n",
    "# Calculate AUC on the test set\n",
    "y_test_lb = lb.transform(y_test)\n",
    "y_test_pred_proba = mlp_classifier.predict_proba(X_test)\n",
    "test_auc = roc_auc_score(y_test_lb, y_test_pred_proba, multi_class='ovr')\n",
    "print(f'Test AUC: {test_auc:.4f}') \n",
    "\n",
    "# Output number of original and predicted type counts for validation set\n",
    "val_original_counts = y_val.value_counts()\n",
    "val_predicted_counts = pd.Series(y_val_pred).value_counts()\n",
    "print('\\nNumber of Original Type Counts (Validation Set):')\n",
    "print(val_original_counts)\n",
    "print('\\nNumber of Predicted Type Counts (Validation Set):')\n",
    "print(val_predicted_counts)\n",
    "\n",
    "# Output number of original and predicted type counts for test set\n",
    "test_original_counts = y_test.value_counts()\n",
    "test_predicted_counts = pd.Series(y_test_pred).value_counts()\n",
    "print('\\nNumber of Original Type Counts (Test Set):')\n",
    "print(test_original_counts)\n",
    "print('\\nNumber of Predicted Type Counts (Test Set):')\n",
    "print(test_predicted_counts)\n",
    "\n",
    "# Calculate precision score\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "print(f'Precision Score: {precision:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC of Gradient Boost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting classifier with the best parameters\n",
    "gb_classifier = GradientBoostingClassifier(learning_rate=0.1, max_depth=5, n_estimators=300, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = gb_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Calculate AUC on the validation set\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_val)\n",
    "y_val_lb = lb.transform(y_val)\n",
    "y_val_pred_proba = gb_classifier.predict_proba(X_val)\n",
    "val_auc = roc_auc_score(y_val_lb, y_val_pred_proba, multi_class='ovr')\n",
    "print(f'Validation AUC: {val_auc:.4f}')\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = gb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)\n",
    "\n",
    "# Calculate AUC on the test set\n",
    "y_test_lb = lb.transform(y_test)\n",
    "y_test_pred_proba = gb_classifier.predict_proba(X_test)\n",
    "test_auc = roc_auc_score(y_test_lb, y_test_pred_proba, multi_class='ovr')\n",
    "print(f'Test AUC: {test_auc:.4f}')\n",
    "\n",
    "# Output number of original and predicted type counts for validation set\n",
    "val_original_counts = y_val.value_counts()\n",
    "val_predicted_counts = pd.Series(y_val_pred).value_counts()\n",
    "print('\\nNumber of Original Type Counts (Validation Set):')\n",
    "print(val_original_counts)\n",
    "print('\\nNumber of Predicted Type Counts (Validation Set):')\n",
    "print(val_predicted_counts)\n",
    "\n",
    "# Output number of original and predicted type counts for test set\n",
    "test_original_counts = y_test.value_counts()\n",
    "test_predicted_counts = pd.Series(y_test_pred).value_counts()\n",
    "print('\\nNumber of Original Type Counts (Test Set):')\n",
    "print(test_original_counts)\n",
    "print('\\nNumber of Predicted Type Counts (Test Set):')\n",
    "print(test_predicted_counts)\n",
    "\n",
    "# Calculate precision score\n",
    "precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "print(f'Precision Score: {precision:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF Testing on Unknown Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Specify the path to your Excel file\n",
    "file_path = '/Users/asia/Desktop/Sir Amar/Test_data.xlsx'\n",
    "\n",
    "# Specify columns to transform\n",
    "columns_to_transform = ['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']\n",
    "\n",
    "# Load the Excel file\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "# Get the names of all sheets in the Excel file\n",
    "sheet_names = xls.sheet_names\n",
    "\n",
    "# Create a dictionary to store the transformed dataframes\n",
    "dfs = {}\n",
    "\n",
    "# Loop over each sheet\n",
    "for sheet_name in sheet_names:\n",
    "    # Read the sheet into a dataframe\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Apply natural logarithm to selected columns\n",
    "    df[columns_to_transform] = np.log(df[columns_to_transform])\n",
    "\n",
    "    # Calculate the mean and standard deviation of the log-transformed columns\n",
    "   # mean = df[columns_to_transform].mean()\n",
    "   # std = df[columns_to_transform].std()\n",
    "\n",
    "    # Standardize the log-transformed columns using the mean and standard deviation\n",
    "  #  df[columns_to_transform] = (df[columns_to_transform] - mean) / std\n",
    "\n",
    "    # Round the values to four decimal places\n",
    "    df = df.round(4)\n",
    "\n",
    "    # Store the transformed dataframe in the dictionary\n",
    "    dfs[sheet_name] = df\n",
    "\n",
    "# Save the transformed dataframes to a new Excel file\n",
    "with pd.ExcelWriter('Pyrite_Standarized_Test_data_file.xlsx') as writer:\n",
    "    for sheet_name, df in dfs.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0.08)\n",
    "X = imputer.fit_transform(X)  # Apply the imputer to the X DataFrame\n",
    "\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n",
    "\n",
    "# Initialize the Random Forest classifier with specified parameters\n",
    "rf_classifier = RandomForestClassifier(n_estimators=400, max_depth=20, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = rf_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)\n",
    "\n",
    "# Load test data from the first sheet\n",
    "test_data1 = pd.read_excel('/Users/asia/Desktop/Sir Amar/Test_data.xlsx', sheet_name='Sample 1')\n",
    "\n",
    "# Apply the same transformations to the test data\n",
    "X_test1 = test_data1[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "X_test1 = imputer.transform(X_test1)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_test_pred1 = rf_classifier.predict(X_test1)\n",
    "\n",
    "# Print the predicted classes\n",
    "print(\"Predicted classes for test data from Sheet1:\")\n",
    "print(y_test_pred1)\n",
    "\n",
    "# Load test data from the second sheet\n",
    "test_data2 = pd.read_excel('/Users/asia/Desktop/Sir Amar/Test_data.xlsx', sheet_name='Sample 2')\n",
    "\n",
    "# Apply the same transformations to the test data\n",
    "X_test2 = test_data2[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "X_test2 = imputer.transform(X_test2)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_test_pred2 = rf_classifier.predict(X_test2)\n",
    "\n",
    "# Print the predicted classes\n",
    "print(\"Predicted classes for test data from Sheet2:\")\n",
    "print(y_test_pred2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n",
    "\n",
    "# Initialize the SVM classifier with specified parameters\n",
    "svm_classifier = SVC(C=100, kernel='rbf', gamma=0.1)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = svm_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)\n",
    "\n",
    "# Load test data from the first sheet\n",
    "test_data1 = pd.read_excel('/Users/asia/Desktop/Sir Amar/Test_data.xlsx', sheet_name='Sample 1')\n",
    "\n",
    "# Apply the same transformations to the test data\n",
    "X_test1 = test_data1[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "X_test1 = imputer.transform(X_test1)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_test_pred1 = svm_classifier.predict(X_test1)\n",
    "\n",
    "# Print the predicted classes\n",
    "print(\"Predicted classes for test data from Sheet1:\")\n",
    "print(y_test_pred1)\n",
    "\n",
    "# Load test data from the second sheet\n",
    "test_data2 = pd.read_excel('/Users/asia/Desktop/Sir Amar/Test_data.xlsx', sheet_name='Sample 2')\n",
    "\n",
    "# Apply the same transformations to the test data\n",
    "X_test2 = test_data2[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "X_test2 = imputer.transform(X_test2)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_test_pred2 = svm_classifier.predict(X_test2)\n",
    "\n",
    "# Print the predicted classes\n",
    "print(\"Predicted classes for test data from Sheet2:\")\n",
    "print(y_test_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0.08)\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n",
    "\n",
    "# Initialize the MLP classifier with specified parameters\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(50, 100, 50), max_iter=300, activation='tanh', solver='adam', alpha=0.0001, learning_rate='constant')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = mlp_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = mlp_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)\n",
    "\n",
    "# Load test data from the first sheet\n",
    "test_data1 = pd.read_excel('/Users/asia/Desktop/Sir Amar/Test_data.xlsx', sheet_name='Sample 1')\n",
    "\n",
    "# Apply the same transformations to the test data\n",
    "X_test1 = test_data1[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "X_test1 = imputer.transform(X_test1)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_test_pred1 = mlp_classifier.predict(X_test1)\n",
    "\n",
    "# Print the predicted classes\n",
    "print(\"Predicted classes for test data from Sheet1:\")\n",
    "print(y_test_pred1)\n",
    "\n",
    "# Load test data from the second sheet\n",
    "test_data2 = pd.read_excel('/Users/asia/Desktop/Sir Amar/Test_data.xlsx', sheet_name='Sample 2')\n",
    "\n",
    "# Apply the same transformations to the test data\n",
    "X_test2 = test_data2[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "X_test2 = imputer.transform(X_test2)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_test_pred2 = mlp_classifier.predict(X_test2)\n",
    "\n",
    "# Print the predicted classes\n",
    "print(\"Predicted classes for test data from Sheet2:\")\n",
    "print(y_test_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0.08)\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n",
    "\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(learning_rate=0.1, max_depth=5, n_estimators=300, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = gb_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = gb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)\n",
    "\n",
    "# Load test data from the first sheet\n",
    "test_data1 = pd.read_excel('/Users/asia/Desktop/Sir Amar/Test_data.xlsx', sheet_name='Sample 1')\n",
    "\n",
    "# Apply the same transformations to the test data\n",
    "X_test1 = test_data1[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "X_test1 = imputer.transform(X_test1)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_test_pred1 = gb_classifier.predict(X_test1)\n",
    "\n",
    "# Print the predicted classes\n",
    "print(\"Predicted classes for test data from Sheet1:\")\n",
    "print(y_test_pred1)\n",
    "\n",
    "# Load test data from the second sheet\n",
    "test_data2 = pd.read_excel('/Users/asia/Desktop/Sir Amar/Test_data.xlsx', sheet_name='Sample 2')\n",
    "\n",
    "# Apply the same transformations to the test data\n",
    "X_test2 = test_data2[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "X_test2 = imputer.transform(X_test2)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_test_pred2 = gb_classifier.predict(X_test2)\n",
    "\n",
    "# Print the predicted classes\n",
    "print(\"Predicted classes for test data from Sheet2:\")\n",
    "print(y_test_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/Users/asia/Desktop/Sir Amar/Pyrite_Standarized_data_file.xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0.08)\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training (60%), validation (20%), and testing sets (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n",
    "\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(learning_rate=0.1, max_depth=5, n_estimators=300, random_state=42)\n",
    "\n",
    "# Apply k-fold cross-validation\n",
    "scores = cross_val_score(gb_classifier, X_train, y_train, cv=15)\n",
    "\n",
    "print(\"Cross-validation scores: \", scores)\n",
    "print(\"Average cross-validation score: \", scores.mean())\n",
    "\n",
    "# Train the classifier on the training data\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = gb_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_rep = classification_report(y_val, y_val_pred)\n",
    "\n",
    "# Output validation set evaluation metrics\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "print('\\nValidation Confusion Matrix:')\n",
    "print(val_conf_matrix)\n",
    "print('\\nValidation Classification Report:')\n",
    "print(val_classification_rep)\n",
    "\n",
    "# Once the model is finalized after validation, evaluate it on the test set\n",
    "y_test_pred = gb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_rep = classification_report(y_test, y_test_pred)\n",
    "\n",
    "# Output test set evaluation metrics\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "print('\\nTest Confusion Matrix:')\n",
    "print(test_conf_matrix)\n",
    "print('\\nTest Classification Report:')\n",
    "print(test_classification_rep)\n",
    "\n",
    "# Load test data from the first sheet\n",
    "test_data1 = pd.read_excel('/Users/asia/Desktop/Sir Amar/Test_data.xlsx', sheet_name='Sample 1')\n",
    "\n",
    "# Apply the same transformations to the test data\n",
    "X_test1 = test_data1[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "X_test1 = imputer.transform(X_test1)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_test_pred1 = gb_classifier.predict(X_test1)\n",
    "\n",
    "# Print the predicted classes\n",
    "print(\"Predicted classes for test data from Sheet1:\")\n",
    "print(y_test_pred1)\n",
    "\n",
    "# Load test data from the second sheet\n",
    "test_data2 = pd.read_excel('/Users/asia/Desktop/Sir Amar/Test_data.xlsx', sheet_name='Sample 2')\n",
    "\n",
    "# Apply the same transformations to the test data\n",
    "X_test2 = test_data2[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "X_test2 = imputer.transform(X_test2)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_test_pred2 = gb_classifier.predict(X_test2)\n",
    "\n",
    "# Print the predicted classes\n",
    "print(\"Predicted classes for test data from Sheet2:\")\n",
    "print(y_test_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/home/alifian/Geology Data Analysis/Pyrite_-21-feb(Final Version).xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "groups = df['Location']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Initialize the Random Forest classifier with specified parameters\n",
    "rf_classifier = RandomForestClassifier(n_estimators=400, max_depth=20, random_state=42)\n",
    "\n",
    "# Initialize LeaveOneGroupOut cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "for train_index, test_index in logo.split(X, y, groups):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    location_train, location_test = groups[train_index], groups[test_index]\n",
    "\n",
    "    # Train the classifier on the training data\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Output evaluation metrics for each location\n",
    "    print(f'Test Location: {location_test.iloc[0]} Accuracy: {accuracy*100:.2f}%')\n",
    "    print('\\nLocation Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    print('\\nLocation Classification Report:')\n",
    "    print(classification_rep)\n",
    "    print('------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/home/alifian/Geology Data Analysis/Pyrite_-21-feb(Final Version).xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "groups = df['Location']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Initialize the SVM classifier with specified parameters\n",
    "svm_classifier = SVC(C=100, kernel='rbf', gamma=0.1, probability=True)\n",
    "\n",
    "# Initialize LeaveOneGroupOut cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "for train_index, test_index in logo.split(X, y, groups):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    location_train, location_test = groups[train_index], groups[test_index]\n",
    "\n",
    "    # Train the classifier on the training data\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Output evaluation metrics for each location\n",
    "    print(f'Test Location: {location_test.iloc[0]}')\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_rep)\n",
    "    print('------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your data from Excel\n",
    "df = pd.read_excel('/home/alifian/Geology Data Analysis/Pyrite_-21-feb(Final Version).xlsx')\n",
    "\n",
    "# Identify features (X) and the target variable (y)\n",
    "X = df[['Co', 'Ni', 'Cu', 'Zn', 'Se', 'Ag', 'Sb', 'Pb', 'Bi', 'As']]\n",
    "y = df['ore-forming fluids Type']\n",
    "groups = df['Location']\n",
    "\n",
    "# Create an imputer object that replaces NaN values with the mean value of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to the X DataFrame\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Initialize the MLP classifier with specified parameters\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(50, 100, 50), max_iter=300, activation='tanh', solver='adam', alpha=0.0001, learning_rate='constant')\n",
    "\n",
    "# Initialize LeaveOneGroupOut cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "for train_index, test_index in logo.split(X, y, groups):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    location_train, location_test = groups[train_index], groups[test_index]\n",
    "\n",
    "    # Train the classifier on the training data\n",
    "    mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = mlp_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Output evaluation metrics for each location\n",
    "    print(f'Test Location: {location_test.iloc[0]} Accuracy: {accuracy*100:.2f}%')\n",
    "    print('\\nLocation Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    print('\\nLocation Classification Report:')\n",
    "    print(classification_rep)\n",
    "    print('------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maths-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
